
HandleWorkerAvailable -> FinishAssignedTask -> FinishAssignedActorTask

/// The worker's ID.
  WorkerID worker_id_;
  /// The worker's PID.
  pid_t pid_;
  /// The language type of this worker.
  Language language_;
  /// Port that this worker listens on.
  /// If port <= 0, this indicates that the worker will not listen to a port.
  int port_;
  /// Connection state of a worker.
  std::shared_ptr<LocalClientConnection> connection_;
  /// The worker's currently assigned task.
  TaskID assigned_task_id_;
  /// Job ID for the worker's current assigned task.
  JobID assigned_job_id_;
  /// The worker's actor ID. If this is nil, then the worker is not an actor.
  ActorID actor_id_;
  /// Whether the worker is dead.
  bool dead_;
  /// Whether the worker is blocked. Workers become blocked in a `ray.get`, if
  /// they require a data dependency while executing a task.
  bool blocked_;
  /// The specific resource IDs that this worker owns for its lifetime. This is
  /// only used for actors.
  ResourceIdSet lifetime_resource_ids_;
  /// The specific resource IDs that this worker currently owns for the duration
  // of a task.
  ResourceIdSet task_resource_ids_;
  std::unordered_set<TaskID> blocked_task_ids_;
  /// The `ClientCallManager` object that is shared by `CoreWorkerClient` from all
  /// workers.
  rpc::ClientCallManager &client_call_manager_;
  /// The rpc client to send tasks to this worker.
  std::unique_ptr<rpc::CoreWorkerClient> rpc_client_;
  /// Whether the worker is detached. This is applies when the worker is actor.
  /// Detached actor means the actor's creator can exit without killing this actor.
  bool is_detached_actor_;
  /// The address of this worker's owner. The owner is the worker that
  /// currently holds the lease on this worker, if any.
  rpc::Address owner_address_;


int64_t expected_task_counter =
        GetExpectedTaskCounter(actor_registry_, spec.ActorId(), spec.CallerId());
    RAY_CHECK(static_cast<int64_t>(spec.ActorCounter()) == expected_task_counter)


FinishAssignTask()

worker->AssignTaskId(spec.TaskId());
worker->SetOwnerAddress(spec.CallerAddress());
worker->AssignJobId(spec.JobId());

    RAY_CHECK(task_dependency_manager_.UnsubscribeGetDependencies(spec.TaskId()));


Bugs:
P0:
- Serialization of certain objects not handled correctly after 0.8.1 (https://github.com/ray-project/ray/issues/7376) - Ryans
- PBT's training_iteration is larger than the stop setting (https://github.com/ray-project/ray/issues/7356) - Richard
- ValueError: Argument must be a dense tensor: ... - got shape [1, 41], but wanted (https://github.com/ray-project/ray/issues/7359) - Sven
- [tune] Hyperparameter tuning of DDPG model/trainer (https://github.com/ray-project/ray/issues/7352) - Richard
- "Cannot perform an interactive login from a non TTY device" when trying to use a private docker registry #7339 - Edward/Alex
- ray object exists in head node, but it cannot be loaded by a remote worker node #7330 - Stephanie

P1:
- PPO PyTorch Bool Multiply Float type Issue (https://github.com/ray-project/ray/issues/7370) - Sven
- [docs] Issue on `rllib.rst` #7345 - Sven/Eric

Features:
P0:
- Do not suggest calling __ray_terminate__ directly (https://github.com/ray-project/ray/issues/7382) - Edward
- ray.services.get_node_ip_address doesn't work well if there is a local proxy #7316 - Edward

P1:
- Ray dashbaord integration (https://github.com/ray-project/ray/issues/7383) - Mitchell
- Actor process leaked after handle is passed to a different task #7326 - Stephani
 
Questions:
- ray logging_monitor steadily increases CPU utilization #7317 -- Philipp





test_args_starkwargs
test_args_named_and_star
test_args_stars_after
test_redefining_remote_functions
test_submit_api
test_many_fractional_resources
test_direct_call_simple
test_call_actors_indirect_through_tasks
test_direct_call_matrix
test_direct_inline_arg_memory_corruption
test_direct_actor_enabled
test_direct_actor_order
test_direct_actor_large_objects
test_direct_actor_pass_by_ref
test_direct_actor_pass_by_ref_order_optimization
test_direct_actor_concurrent
test_wait


DispatchTasks
  AssignTask
    FinishAssignTask(3)

FinishAssignTask(2)

void NodeManager::AssignTask(const std::shared_ptr<Worker> &worker, const Task &task,
                             std::vector<std::function<void()>> *post_assign_callbacks) {
  const TaskSpecification &spec = task.GetTaskSpecification();
  RAY_CHECK(post_assign_callbacks);

  // If this is an actor task, check that the new task has the correct counter.
  if (spec.IsActorTask()) {
    // An actor task should only be ready to be assigned if it matches the
    // expected task counter.
    int64_t expected_task_counter =
        GetExpectedTaskCounter(actor_registry_, spec.ActorId(), spec.CallerId());
    RAY_CHECK(static_cast<int64_t>(spec.ActorCounter()) == expected_task_counter)
        << "Expected actor counter: " << expected_task_counter << ", task "
        << spec.TaskId() << " has: " << spec.ActorCounter();
  }

  RAY_LOG(DEBUG) << "Assigning task " << spec.TaskId() << " to worker with pid "
                 << worker->Pid() << ", worker id: " << worker->WorkerId();
  flatbuffers::FlatBufferBuilder fbb;

  // Resource accounting: acquire resources for the assigned task.
  auto acquired_resources =
      local_available_resources_.Acquire(spec.GetRequiredResources());
  cluster_resource_map_[self_node_id_].Acquire(spec.GetRequiredResources());

  if (spec.IsActorCreationTask()) {
    // Check that the actor's placement resource requirements are satisfied.
    RAY_CHECK(spec.GetRequiredPlacementResources().IsSubset(
        cluster_resource_map_[self_node_id_].GetTotalResources()));
    worker->SetLifetimeResourceIds(acquired_resources);
  } else {
    worker->SetTaskResourceIds(acquired_resources);
  }

  auto task_id = spec.TaskId();
  if (task.OnDispatch() != nullptr) {
    if (task.GetTaskSpecification().IsDetachedActor()) {
      worker->MarkDetachedActor();
    }

    const auto owner_worker_id = WorkerID::FromBinary(spec.CallerAddress().worker_id());
    const auto owner_node_id = ClientID::FromBinary(spec.CallerAddress().raylet_id());
    RAY_CHECK(!owner_worker_id.IsNil());
    RAY_LOG(DEBUG) << "Worker lease request DISPATCH " << task_id << " to worker "
                   << worker->WorkerId() << ", owner ID " << owner_worker_id;

    task.OnDispatch()(worker, initial_config_.node_manager_address, worker->Port(),
                      worker->WorkerId(),
                      spec.IsActorCreationTask() ? worker->GetLifetimeResourceIds()
                                                 : worker->GetTaskResourceIds());

    // If the owner has died since this task was queued, cancel the task by
    // killing the worker.
    if (failed_workers_cache_.count(owner_worker_id) > 0 ||
        failed_nodes_cache_.count(owner_node_id) > 0) {
      // TODO(swang): Skip assigning this task to this worker instead of
      // killing the worker?
      KillWorker(worker);
    }

    post_assign_callbacks->push_back([this, worker, task_id]() {
      RAY_LOG(DEBUG) << "Finished assigning task " << task_id << " to worker "
                     << worker->WorkerId();

      FinishAssignTask(worker, task_id, /*success=*/true);
    });
  } else {
    ResourceIdSet resource_id_set =
        worker->GetTaskResourceIds().Plus(worker->GetLifetimeResourceIds());
    if (worker->AssignTask(task, resource_id_set).ok()) {
      RAY_LOG(DEBUG) << "Assigned task " << task_id << " to worker "
                     << worker->WorkerId();
      post_assign_callbacks->push_back([this, worker, task_id]() {
        FinishAssignTask(worker, task_id, /*success=*/true);
      });
    } else {
      RAY_LOG(ERROR) << "Failed to assign task " << task_id << " to worker "
                     << worker->WorkerId() << ", disconnecting client";
      post_assign_callbacks->push_back([this, worker, task_id]() {
        FinishAssignTask(worker, task_id, /*success=*/false);
      });
    }
  }
}


void NodeManager::FinishAssignTask(const std::shared_ptr<Worker> &worker,
                                   const TaskID &task_id, bool success) {
  RAY_LOG(DEBUG) << "FinishAssignTask: " << task_id;
  // Remove the ASSIGNED task from the READY queue.
  Task assigned_task;
  TaskState state;
  if (!local_queues_.RemoveTask(task_id, &assigned_task, &state)) {
    // TODO(edoakes): should we be failing silently here?
    return;
  }
  RAY_CHECK(state == TaskState::READY);
  if (success) {
    auto spec = assigned_task.GetTaskSpecification();
    // We successfully assigned the task to the worker.
    worker->AssignTaskId(spec.TaskId());
    RAY_LOG(WARNING) << "FinishAssignTask 1, " << worker->WorkerId() << ", " << spec.CallerAddress().worker_id();
    worker->SetOwnerAddress(spec.CallerAddress());
    worker->AssignJobId(spec.JobId());
    // TODO(swang): For actors with multiple actor handles, to
    // guarantee that tasks are replayed in the same order after a
    // failure, we must update the task's execution dependency to be
    // the actor's current execution dependency.

    // Mark the task as running.
    // (See design_docs/task_states.rst for the state transition diagram.)
    local_queues_.QueueTasks({assigned_task}, TaskState::RUNNING);
    // Notify the task dependency manager that we no longer need this task's
    // object dependencies.
    RAY_CHECK(task_dependency_manager_.UnsubscribeGetDependencies(spec.TaskId()));
  } else {
    RAY_LOG(WARNING) << "Failed to send task to worker, disconnecting client";
    // We failed to send the task to the worker, so disconnect the worker.
    RAY_LOG(WARNING) << "ProcessDisconnectClientMessage 3";
    ProcessDisconnectClientMessage(worker->Connection());
    // Queue this task for future assignment. We need to do this since
    // DispatchTasks() removed it from the ready queue. The task will be
    // assigned to a worker once one becomes available.
    // (See design_docs/task_states.rst for the state transition diagram.)
    local_queues_.QueueTasks({assigned_task}, TaskState::READY);
    DispatchTasks(MakeTasksByClass({assigned_task}));
  }
}

  RAY_LOG(DEBUG) << "FinishAssignTask: " << task_id;
  // Remove the ASSIGNED task from the READY queue.
  Task assigned_task;
  TaskState state;
  if (!local_queues_.RemoveTask(task_id, &assigned_task, &state)) {
    // TODO(edoakes): should we be failing silently here?
    return;
  }
  RAY_CHECK(state == TaskState::READY);
  if (success) {
    auto spec = assigned_task.GetTaskSpecification();
    // We successfully assigned the task to the worker.
    worker->AssignTaskId(spec.TaskId());
    RAY_LOG(WARNING) << "FinishAssignTask 1, " << worker->WorkerId() << ", " << spec.CallerAddress().worker_id();
    worker->SetOwnerAddress(spec.CallerAddress());
    worker->AssignJobId(spec.JobId());


W0219 08:47:49.423257 143726016 node_manager.cc:575] ====x ResourceCreateUpdated -> UpdateResourceCapacity ====
W0219 08:47:49.423391 143726016 node_manager.cc:575] ====x ResourceCreateUpdated -> UpdateResourceCapacity ====
W0219 08:47:49.423502 143726016 node_manager.cc:575] ====x ResourceCreateUpdated -> UpdateResourceCapacity ====

W0219 08:47:49.598166 143726016 node_manager.cc:1582] ====x HandleRequestWorkerLease->NewSchedulerSchedulePendingTasks ====
W0219 08:47:50.408880 143726016 node_manager.cc:1093] ====x HandleWorkerAvailable -> DispatchScheduledTasksToWorkers ====
W0219 08:47:50.408973 143726016 node_manager.cc:1449] ====x DispatchScheduledTasksToWorkers -> AllocateLocalTaskResources ====
W0219 08:47:50.415704 143726016 node_manager.cc:1650] ====x HandleReturnWorker -> FreeLocalTaskResources ====

W0219 08:47:50.415794 143726016 node_manager.cc:1093] ====x HandleWorkerAvailable -> DispatchScheduledTasksToWorkers ====
W0219 08:47:50.417316 143726016 node_manager.cc:1582] ====x HandleRequestWorkerLease->NewSchedulerSchedulePendingTasks ====
W0219 08:47:50.417342 143726016 node_manager.cc:1449] ====x DispatchScheduledTasksToWorkers -> AllocateLocalTaskResources ====
W0219 08:47:50.417954 143726016 node_manager.cc:1650] ====x HandleReturnWorker -> FreeLocalTaskResources ====

W0219 08:47:50.418031 143726016 node_manager.cc:1093] ====x HandleWorkerAvailable -> DispatchScheduledTasksToWorkers ====
W0219 08:47:50.511652 143726016 node_manager.cc:1582] ====x HandleRequestWorkerLease->NewSchedulerSchedulePendingTasks ====
W0219 08:47:50.511687 143726016 node_manager.cc:1449] ====x DispatchScheduledTasksToWorkers -> AllocateLocalTaskResources ====
W0219 08:47:50.512508 143726016 node_manager.cc:1650] ====x HandleReturnWorker -> FreeLocalTaskResources ====

W0219 08:47:50.512634 143726016 node_manager.cc:1093] ====x HandleWorkerAvailable -> DispatchScheduledTasksToWorkers ====
W0219 08:47:50.823174 143726016 node_manager.cc:1582] ====x HandleRequestWorkerLease->NewSchedulerSchedulePendingTasks ====
W0219 08:47:50.823212 143726016 node_manager.cc:1449] ====x DispatchScheduledTasksToWorkers -> AllocateLocalTaskResources ====
W0219 08:47:50.824156 143726016 node_manager.cc:1650] ====x HandleReturnWorker -> FreeLocalTaskResources ====

W0219 08:47:50.824326 143726016 node_manager.cc:1093] ====x HandleWorkerAvailable -> DispatchScheduledTasksToWorkers ====
W0219 08:47:50.824856 143726016 node_manager.cc:1582] ====x HandleRequestWorkerLease->NewSchedulerSchedulePendingTasks ====
W0219 08:47:50.824895 143726016 node_manager.cc:1449] ====x DispatchScheduledTasksToWorkers -> AllocateLocalTaskResources ====
W0219 08:47:50.825748 143726016 node_manager.cc:1650] ====x HandleReturnWorker -> FreeLocalTaskResources ====

W0219 08:47:50.825884 143726016 node_manager.cc:1093] ====x HandleWorkerAvailable -> DispatchScheduledTasksToWorkers ====
W0219 08:47:50.920161 143726016 node_manager.cc:1582] ====x HandleRequestWorkerLease->NewSchedulerSchedulePendingTasks ====
W0219 08:47:50.920202 143726016 node_manager.cc:1449] ====x DispatchScheduledTasksToWorkers -> AllocateLocalTaskResources ====
W0219 08:47:50.921074 143726016 node_manager.cc:1650] ====x HandleReturnWorker -> FreeLocalTaskResources ====

W0219 08:47:50.921177 143726016 node_manager.cc:1093] ====x HandleWorkerAvailable -> DispatchScheduledTasksToWorkers ====
W0219 08:47:51.022632 143726016 node_manager.cc:1582] ====x HandleRequestWorkerLease->NewSchedulerSchedulePendingTasks ====
W0219 08:47:51.022662 143726016 node_manager.cc:1449] ====x DispatchScheduledTasksToWorkers -> AllocateLocalTaskResources ====
W0219 08:47:51.023331 143726016 node_manager.cc:1650] ====x HandleReturnWorker -> FreeLocalTaskResources ====

W0219 08:47:51.023416 143726016 node_manager.cc:1093] ====x HandleWorkerAvailable -> DispatchScheduledTasksToWorkers ====
W0219 08:47:51.125753 143726016 node_manager.cc:1582] ====x HandleRequestWorkerLease->NewSchedulerSchedulePendingTasks ====
W0219 08:47:51.125784 143726016 node_manager.cc:1449] ====x DispatchScheduledTasksToWorkers -> AllocateLocalTaskResources ====
W0219 08:47:51.126543 143726016 node_manager.cc:1650] ====x HandleReturnWorker -> FreeLocalTaskResources ====

W0219 08:47:51.126631 143726016 node_manager.cc:1093] ====x HandleWorkerAvailable -> DispatchScheduledTasksToWorkers ====
W0219 08:47:51.228188 143726016 node_manager.cc:1582] ====x HandleRequestWorkerLease->NewSchedulerSchedulePendingTasks ====
W0219 08:47:51.228219 143726016 node_manager.cc:1449] ====x DispatchScheduledTasksToWorkers -> AllocateLocalTaskResources ====
W0219 08:47:51.229769 143726016 node_manager.cc:1650] ====x HandleReturnWorker -> FreeLocalTaskResources ====

W0219 08:47:51.229877 143726016 node_manager.cc:1093] ====x HandleWorkerAvailable -> DispatchScheduledTasksToWorkers ====
W0219 08:47:51.332478 143726016 node_manager.cc:1582] ====x HandleRequestWorkerLease->NewSchedulerSchedulePendingTasks ====
W0219 08:47:51.332525 143726016 node_manager.cc:1449] ====x DispatchScheduledTasksToWorkers -> AllocateLocalTaskResources ====
W0219 08:47:51.333431 143726016 node_manager.cc:1650] ====x HandleReturnWorker -> FreeLocalTaskResources ====

W0219 08:47:51.333657 143726016 node_manager.cc:1093] ====x HandleWorkerAvailable -> DispatchScheduledTasksToWorkers ====
